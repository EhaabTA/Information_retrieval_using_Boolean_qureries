{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36ae521-cd63-4b7f-a833-91873081d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter any query :  artificial OR intelligence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The terms obtained by spliting th user query is ['artificial', 'intelligence']\n",
      "The inverted index if the :artificial OR intelligence is [1, 2, 3, 7, 8, 12, 15, 16, 17, 21, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "#nltk.download('punkt')\n",
    "\n",
    "stopwords = []\n",
    "dictionary = {}\n",
    "\n",
    "\n",
    "def read_stopwords(filename):\n",
    "    file_read = open(filename,'r')\n",
    "    content = file_read.read();\n",
    "    file_read.close()\n",
    "    stopwords = content.split('\\n')\n",
    "    return stopwords    \n",
    "\n",
    "def tokenizer(fileContent):\n",
    "    fileContent = fileContent.lower()\n",
    "    fileContent = fileContent.replace(\"-\", \" \")  # replace '-' with simple space\n",
    "    fileContent = fileContent.replace(\"â€¢\", \" \")  # replace '.' with simple space\n",
    "    fileContent = re.sub(r'https?://(?:www\\.)?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', fileContent)  # remove urls\n",
    "    fileContent = re.sub(r'\\S+\\.com\\b', '', fileContent)  # remove .com\n",
    "    fileContent = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', fileContent)  # remove emails\n",
    "    fileContent = re.sub(r'\\b\\d+\\b', '', fileContent)  # remove numbers\n",
    "    fileContent = re.sub(r'[^\\w\\s]', '', fileContent)  # remove other useless punctuation\n",
    "    words = re.split(r'\\s+|\\n+', fileContent)\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "docs_directory = \"ResearchPapers\"\n",
    "\n",
    "def clean_and_generate_positions(docs_directory): #cleaning the terms and creating the inverted indexes\n",
    "    # Build inverted index\n",
    "    inverted_index = {}\n",
    "\n",
    "    file_pattern = '*.txt'\n",
    "\n",
    "    # Use glob to find all files that match the pattern in the folder\n",
    "    file_list = glob.glob(os.path.join(docs_directory, file_pattern))\n",
    "    \n",
    "    # Loop over each file in the list and read its contents\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, 'r') as file:\n",
    "            document_content = file.read()\n",
    "            file_name = os.path.basename(file_path).split('.')[0]\n",
    "            # Preprocess the document\n",
    "            processed_words = tokenizer(document_content)\n",
    "            # Update inverted index\n",
    "            for position, term in enumerate(processed_words):\n",
    "                if term not in stopwords:\n",
    "                    if term not in inverted_index:\n",
    "                        inverted_index[term] = {int(file_name): [position]}\n",
    "                    else:\n",
    "                        if int(file_name) not in inverted_index[term]:\n",
    "                            inverted_index[term][int(file_name)] = [position]\n",
    "                        else:\n",
    "                            inverted_index[term][int(file_name)].append(position)\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def boolean_and(query_terms, inverted_index):\n",
    "    print(\"\\nThe terms obtained by spliting th user query is\",query_terms)\n",
    "    result = None\n",
    "    common_docs = set()\n",
    "    for term in query_terms:\n",
    "        stemmed_term = stemmer.stem(term)\n",
    "        postings = set(inverted_index.get(stemmed_term, {}).keys())\n",
    "        if result is None:\n",
    "            result = postings\n",
    "        else:\n",
    "            # Take the intersection of postings based on document name only\n",
    "            common_docs = common_docs.union(postings)\n",
    "    # Convert the result to a sorted list by docID\n",
    "    sorted_result = sorted(list(common_docs), key=lambda x: (int(x.split('.')[0]) if '.' in x else int(x)) if isinstance(x, str) else x)\n",
    "\n",
    "    return sorted_result\n",
    "\n",
    "\n",
    "def boolean_or(query_terms, inverted_index):\n",
    "    result = set()\n",
    "    print(\"\\nThe terms obtained by spliting th user query is\",query_terms)\n",
    "    for term in query_terms:\n",
    "        stemmed_term = stemmer.stem(term)\n",
    "        postings = set(inverted_index.get(stemmed_term, []))\n",
    "        result.update(postings)\n",
    "\n",
    "    result = list(result)\n",
    "\n",
    "    sorted_result = sorted(result, key=lambda x: (int(x.split('.')[0]) if '.' in x else int(x)) if isinstance(x, str) else x)\n",
    "\n",
    "    return sorted_result\n",
    "\n",
    "def boolean_not(not_query_terms, inverted_index):\n",
    "    result = set()\n",
    "    print(\"The terms obtained by spliting th user query is\",not_query_terms)\n",
    "    # Find all documents\n",
    "    all_docs = set()\n",
    "    for postings in inverted_index.values():\n",
    "        all_docs.update(postings.keys())\n",
    "    \n",
    "    # Find documents containing any of the terms from the NOT query\n",
    "    not_docs = set()\n",
    "    for term in not_query_terms:\n",
    "        stemmed_term = stemmer.stem(term)\n",
    "        postings = set(inverted_index.get(stemmed_term, {}).keys())\n",
    "        not_docs.update(postings)\n",
    "    \n",
    "    # Find documents not containing any of the terms from the NOT query\n",
    "    for doc_id in all_docs:\n",
    "        if str(doc_id) + '.txt' not in not_docs and str(doc_id) + '.txt' != \"Stopword-List.txt\":\n",
    "            result.add(str(doc_id) + '.txt')\n",
    "    \n",
    "    # Sort the result by docID\n",
    "    sorted_result = sorted(result, key=lambda x: int(x.split('.')[0]) if '.' in x else x)\n",
    "\n",
    "    return sorted_result\n",
    "\n",
    "def process_boolean_query(user_query, inverted_index):\n",
    "    # Tokenize the user input into a list of terms\n",
    "    query_terms = user_query.split()\n",
    "\n",
    "    # Define lists to store subqueries and operators\n",
    "    subqueries = []\n",
    "    operators = []\n",
    "\n",
    "    # Split the query into subqueries and operators\n",
    "    for term in query_terms:\n",
    "        if term.upper() in {'AND', 'OR', 'NOT'}:\n",
    "            operators.append(term.upper())\n",
    "        else:\n",
    "            subqueries.append(term)\n",
    "\n",
    "    # Evaluate the subqueries based on operators\n",
    "    result = []\n",
    "\n",
    "    if not operators:\n",
    "        for term in subqueries:\n",
    "            result.extend(inverted_index.get(term, []))\n",
    "        return result\n",
    "\n",
    "    # Iterate through operators and apply boolean operations\n",
    "    for i, operator in enumerate(operators):\n",
    "        if operator == 'AND':\n",
    "            i = i + 2\n",
    "            result = boolean_and(subqueries[:i], inverted_index)\n",
    "        elif operator == 'OR':\n",
    "            result = boolean_or(subqueries, inverted_index)\n",
    "        elif operator == 'NOT':\n",
    "            result = boolean_not(subqueries, inverted_index)\n",
    "            result = [doc.replace('.txt', '') for doc in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #reading stopwords from stopwords file\n",
    "    #read_stopwords function takes filename as an input and return the list of stopwords\n",
    "    stopwords = read_stopwords('Stopword-List.txt')\n",
    "\n",
    "    #read_all_files takes the stopwords as a parameter and reads all the files in the folder with a .txt extension and call tokenize method\n",
    "    \n",
    "    \n",
    "    #dictionary is sorted first on keys and then on document id's\n",
    "    #sorted_dictionary = sort_dictionary(dictionary)\n",
    "    inverted_index = clean_and_generate_positions(docs_directory)\n",
    "    # Printing the first 10 terms from the sorted dictionary\n",
    "    \"\"\"\n",
    " print(\"Inverted index : \")\n",
    "    for term in inverted_index.items():\n",
    "        print(term, \":\", postings)\n",
    "        count += 1\n",
    "        if count == 1000:\n",
    "            break \n",
    "            \n",
    "    # Example AND query\n",
    "    and_query = ['feature', 'selection','redundancy']\n",
    "    result_and = boolean_and(and_query, inverted_index)\n",
    "\n",
    "    # Print the sorted result\n",
    "    print(f\" AND Query Result: {result_and}\")\n",
    "    \n",
    "    \n",
    "    # Example OR query\n",
    "    or_query = ['transformer', 'model']\n",
    "    result_or = boolean_or(or_query, inverted_index)\n",
    "\n",
    "    print(f\"OR Query Result: {result_or}\")\n",
    "    \n",
    "    \n",
    "    #Example NOT query\n",
    "    not_query = ['cancer', 'feature']\n",
    "    result_not = boolean_not(not_query, inverted_index)\n",
    "\n",
    "    result_not = [doc.replace('.txt', '') for doc in result_not]\n",
    "    # Print the result without \"txt\" extension\n",
    "    print(f\"NOT Query Result: {result_not}\")\n",
    "    \n",
    "    \n",
    "     #taking the user query\n",
    "    user_query = input(\"Enter any query : \")\n",
    "    result = process_boolean_query(user_query, inverted_index)\n",
    "    print(f\"The inverted index if the :{user_query} is\",result)\n",
    "    \n",
    "\"\"\"\n",
    "    #print(inverted_index)\n",
    "    user_query = input(\"Enter any query : \")\n",
    "    result = process_boolean_query(user_query, inverted_index)\n",
    "    print(f\"The inverted index if the :{user_query} is\",result)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
